{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert用于文本分类任务的应用步骤\n",
    "1. 预训练阶段<br>\n",
    "Bert通过大规模的无标签文本数据进行预训练，学习了丰富的语言表示\n",
    "* 掩码语言模型（Masked Language Model，MLM）：随机遮掩输入文本中的一些单词，模型的任务就是预测被遮掩的单词，这使得模型能够理解上下文信息；\n",
    "* 下一个句子预测（Next Sentence Prediction，NSP）：输入两个句子，模型需要判断第二个句子是否是第一个句子的后续。\n",
    "\n",
    "2. 微调阶段<br>\n",
    "在预训练之后，可以通过将Bert微调来适应特定的文本分类任务\n",
    "* 数据准备：准备好带标签的数据集，包括文本及其对应的分类标签\n",
    "* 模型构建：使用`BertForSequenceClassification`等类加载预训练的Bert模型，并提那家一个分类层（全连接层）来输出分类结果\n",
    "* 输入格式化：将文本数据转换为Bert可以接受的格式\n",
    "    * Tokenization：使用Bert的分词器将文本转换为token ID，并进行填充\n",
    "    * Attention Mask：生成一个Mask，指示模型应该关注哪些token（1表示有效token，0表示填充token）\n",
    "\n",
    "3. 训练<br>\n",
    "在微调过程中，通过使用带标签的训练数据对模型进行训练。模型会根据输入文本和相应的标签调整其权重，以最大化正确分类的概率。使用损失函数（如交叉熵损失）来评估模型性能。\n",
    "\n",
    "4. 评估与预测<br>\n",
    "在训练完成后，可以使用验证集或测试集对模型进行评估，以确定其分类性能。模型可以输出每个类别的概率分布，用户可以根据最大概率进行类别判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# BertForSequenceClassification：Hugging Face transformers库中的一个类，专门用于处理序列分类任务，例如情感分析、主题分类等\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载预训练的BERT模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"  # 可以根据需要选择不同的BERT版本\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # 分词器\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 适用于二分类\n",
    "# :param model_name: 指定要加载的预训练BERT模型的名称或者路径\n",
    "# :param num_labels: 指定模型的输出标签数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集，这里使用的是 Hugging Face 自带的 dataset 库\n",
    "dataset = load_dataset(\"imdb\")  # 示例是IMDB影评分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n"
     ]
    }
   ],
   "source": [
    "df_train=dataset['train'].to_pandas()\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n"
     ]
    }
   ],
   "source": [
    "df_test=dataset['train'].to_pandas()\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理函数，将文本转化为模型输入格式\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行分词和编码\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 模型定义与编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据集中抽取一部分样本用于debug模式\n",
    "small_train_dataset = encoded_dataset['train'].select(range(100))  # 抽取100个样本作为训练集\n",
    "small_eval_dataset = encoded_dataset['test'].select(range(100))    # 抽取100个样本作为验证集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出结果保存的路径\n",
    "    evaluation_strategy=\"epoch\",     # 在每个epoch后进行验证\n",
    "    per_device_train_batch_size=16,  # 训练批量大小\n",
    "    per_device_eval_batch_size=64,   # 验证批量大小\n",
    "    num_train_epochs=3,              # 训练的epoch数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Trainer API来进行训练和验证\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 要训练的模型\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=small_train_dataset,  # 训练集\n",
    "    eval_dataset=small_eval_dataset,    # 验证集\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [00:22<00:32,  2.34s/it]\n",
      " 33%|███▎      | 7/21 [00:27<00:32,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09301885962486267, 'eval_runtime': 4.9323, 'eval_samples_per_second': 20.275, 'eval_steps_per_second': 0.405, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [00:41<00:13,  1.86s/it]\n",
      " 67%|██████▋   | 14/21 [00:46<00:13,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.028304338455200195, 'eval_runtime': 4.9948, 'eval_samples_per_second': 20.021, 'eval_steps_per_second': 0.4, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 21/21 [01:09<00:00,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01811107248067856, 'eval_runtime': 6.7211, 'eval_samples_per_second': 14.879, 'eval_steps_per_second': 0.298, 'epoch': 3.0}\n",
      "{'train_runtime': 69.3774, 'train_samples_per_second': 4.324, 'train_steps_per_second': 0.303, 'train_loss': 0.13575719651721774, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=0.13575719651721774, metrics={'train_runtime': 69.3774, 'train_samples_per_second': 4.324, 'train_steps_per_second': 0.303, 'total_flos': 78933316608000.0, 'train_loss': 0.13575719651721774, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_text_classification/tokenizer_config.json',\n",
       " './bert_text_classification/special_tokens_map.json',\n",
       " './bert_text_classification/vocab.txt',\n",
       " './bert_text_classification/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "model.save_pretrained('./bert_text_classification')\n",
    "tokenizer.save_pretrained('./bert_text_classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
