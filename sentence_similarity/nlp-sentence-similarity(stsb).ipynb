{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert用于句子相似度的应用步骤\n",
    "在句子相似度任务中，主要目标是通过模型对两个句子的相似度进行建模，并且输出一个连续的相似性分数，通常是0到5之间，数值越大表示句子越相似。\n",
    "\n",
    "1. 任务定义\n",
    "* 任务是给定两个句子，模型输出它们的相似度分数。\n",
    "2. 数据准备\n",
    "* 为了训练模型，通常需要一个包含句子对和相应相似度分数的数据集。句子对会作为输入，目标是让模型预测的分数接近数据集中标注的相似度分数。\n",
    "3. 模型选择\n",
    "* 我们使用预训练的Bert模型，它已经在大量文本上进行了训练，可以很好的处理自然语言理解任务。Bert可以将两个句子作为输入，并通过模型进行处理，提取它们的语义。\n",
    "4. 输入处理\n",
    "* Bert模型需要将两个句子拼接在一起作为输入，并且在它们中间用特殊标记`[SET]`分开。\n",
    "* 输入的形式通常是：`[CLS]句子1[SEP]句子2[SEP]`。其中`[CLS]`是一个特殊的分类标记，表示整个句子的语义摘要；而`[SEP]`用来分隔句子\n",
    "5. 模型输出\n",
    "* BERT会对输入的句子进行编码，并生成一个表示句子对相似性的向量。然后，我们在Bert输出的基础上添加一个回归层。回归层的作用是将这个向量转换成一个相似度分数。\n",
    "6. 损失函数\n",
    "* 我们会计算模型预测的相似度分数与数据集中真实标注分数之间的差异。我们使用均方误差（MSE）作为函数。通过反向传播和优化起，模型会不断调整参数，使得损失值减小，也就是模型的预测越来越准确。\n",
    "7. 训练过程\n",
    "8. 评估模型\n",
    "* 在验证和测试阶段，我们通过将模型预测的相似度分数和真实分数进行比较来评估模型性能。通常也是通过均方误差来衡量模型的准确性。均方误差越小，模型的预测就越准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载Hugging Face数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 STS-B 数据集\n",
    "dataset = load_dataset(\"glue\", \"stsb\")\n",
    "# sts-b是一个句子相似度的基准任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 5749\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1379\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sentence1  \\\n",
      "0                         A plane is taking off.   \n",
      "1                A man is playing a large flute.   \n",
      "2  A man is spreading shreded cheese on a pizza.   \n",
      "3                   Three men are playing chess.   \n",
      "4                    A man is playing the cello.   \n",
      "\n",
      "                                           sentence2  label  idx  \n",
      "0                        An air plane is taking off.   5.00    0  \n",
      "1                          A man is playing a flute.   3.80    1  \n",
      "2  A man is spreading shredded cheese on an uncoo...   3.80    2  \n",
      "3                         Two men are playing chess.   2.60    3  \n",
      "4                 A man seated is playing the cello.   4.25    4  \n"
     ]
    }
   ],
   "source": [
    "df_train=dataset['train'].to_pandas()\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sentence1  \\\n",
      "0                    A girl is styling her hair.   \n",
      "1       A group of men play soccer on the beach.   \n",
      "2  One woman is measuring another woman's ankle.   \n",
      "3                A man is cutting up a cucumber.   \n",
      "4                       A man is playing a harp.   \n",
      "\n",
      "                                          sentence2  label  idx  \n",
      "0                      A girl is brushing her hair.   -1.0    0  \n",
      "1  A group of boys are playing soccer on the beach.   -1.0    1  \n",
      "2           A woman measures another woman's ankle.   -1.0    2  \n",
      "3                      A man is slicing a cucumber.   -1.0    3  \n",
      "4                      A man is playing a keyboard.   -1.0    4  \n"
     ]
    }
   ],
   "source": [
    "df_test=dataset['test'].to_pandas()\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义数据集的预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "        使用Bert的tokenizer对句子进行编码，生成input_ids和attention_mask\n",
    "    \"\"\"\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对训练集和验证集进行tokenization\n",
    "# map方法将tokenize_function应用于训练和验证数据集\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置数据集的格式为 PyTorch tensors\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 准备DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练集和验证集 DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练的BERT模型，指定类别数为1，因为STS-B是回归任务\n",
    "# num_labels=1表示这是一个单维度回归任务（句子相似度得分在0到5之间）\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 使用Adam优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "def train(model, dataloader, optimizer, num_epochs=3):\n",
    "    model.train()\n",
    "    loss_fn = torch.nn.MSELoss()  # 回归任务的损失函数使用均方误差\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label'].unsqueeze(1)  # 调整label形状\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.30950252380636\n",
      "Epoch 2/3, Loss: 0.4926092455370559\n",
      "Epoch 3/3, Loss: 0.32483585472736093\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train(model, train_dataloader, optimizer, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            label = batch['label'].unsqueeze(1)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds.append(outputs.logits.cpu().numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "    \n",
    "    # 将预测和真实值进行拼接\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    labels = np.concatenate(labels).flatten()\n",
    "    \n",
    "    # 计算均方误差（MSE）\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    print(f\"Validation MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.5731\n"
     ]
    }
   ],
   "source": [
    "# 评估验证集上的表现\n",
    "evaluate(model, valid_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
